{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:38:37.468698Z",
     "start_time": "2025-06-04T13:38:26.787814Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import libraries and utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import tensorflow as tf\n",
    "\n",
    "from src.utils import (load_all, aggregate_player_stats, get_latest_valuation,\n",
    "                      merge_player_data, fillna_and_scale, encode_categorical,\n",
    "                      scatter_actual_vs_pred, compute_age)\n",
    "\n",
    "pd.set_option('display.max_columns', 100)"
   ],
   "id": "9687a6cf01e7bbba",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:38:48.847683Z",
     "start_time": "2025-06-04T13:38:39.985277Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ─── 1. Build & clean main_df ─────────────────────────────────────────────────\n",
    "data = load_all()\n",
    "# Example: print available tables\n",
    "for name, df in data.items():\n",
    "    print(f'{name}: {df.shape}')\n",
    "    \n",
    "stats      = aggregate_player_stats(data['appearances'])\n",
    "latest_val = get_latest_valuation(data['player_valuations'])\n",
    "main_df    = merge_player_data(data['players'], stats, latest_val)\n",
    "\n"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "appearances: (1706806, 13)\n",
      "clubs: (439, 17)\n",
      "club_games: (148052, 11)\n",
      "competitions: (44, 11)\n",
      "games: (74026, 23)\n",
      "game_events: (1035043, 10)\n",
      "game_lineups: (2191911, 10)\n",
      "players: (32601, 23)\n",
      "player_valuations: (496606, 5)\n",
      "transfers: (79646, 10)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:38:50.943974Z",
     "start_time": "2025-06-04T13:38:50.865588Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Remove the duplicated market_value column, rename the one we need\n",
    "main_df = main_df.drop(columns='market_value_in_eur_x') \\\n",
    "                 .rename(columns={'market_value_in_eur_y': 'market_value_in_eur'})\n",
    "\n",
    "# Compute age (assign result back into main_df)\n",
    "main_df = compute_age(main_df)\n",
    "\n",
    "# Drop any row where our target is missing\n",
    "main_df = main_df.dropna(subset=['market_value_in_eur'])\n",
    "\n"
   ],
   "id": "62576eb3e3af0d90",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:38:52.630673Z",
     "start_time": "2025-06-04T13:38:52.611060Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# ─── 2. Decide which raw columns to drop (IDs, free‐form text, dates we’ve already engineered) ──\n",
    "\n",
    "drop_cols = [\n",
    "    'player_id',\n",
    "    'player_code',\n",
    "    'first_name', 'last_name', 'name', 'player_name',\n",
    "    'url', 'image_url',\n",
    "    'date_of_birth',          # we used this to create 'age'\n",
    "    'contract_expiration_date',\n",
    "    'agent_name',\n",
    "    'current_club_name',      # free‐form text\n",
    "    'from_club_name', 'to_club_name',\n",
    "    # …add any other pure‐text or uninterpretable strings here if needed…\n",
    "]\n",
    "\n",
    "# Everything except drop_cols + the target itself\n",
    "all_cols    = main_df.columns.tolist()\n",
    "to_exclude  = drop_cols + ['market_value_in_eur']\n",
    "feature_cols = [c for c in all_cols if c not in to_exclude]\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "43c139e37e0b8a2",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:38:59.342176Z",
     "start_time": "2025-06-04T13:38:59.323916Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# ─── 3. Auto-detect numeric vs. categorical among feature_cols ─────────────────────\n",
    "\n",
    "# 3a. Numeric columns (int/float/Int64)\n",
    "num_cols = main_df[feature_cols].select_dtypes(include=['int64','float64','Int64']).columns.tolist()\n",
    "\n",
    "# 3b. Categorical columns (object or category)\n",
    "cat_cols = main_df[feature_cols].select_dtypes(include=['object','category']).columns.tolist()\n",
    "\n",
    "print(\"Numeric columns that will be scaled:\", num_cols)\n",
    "print(\"Categorical columns that will be one‐hot’ed:\", cat_cols)"
   ],
   "id": "ef8e479882f0eabc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric columns that will be scaled: ['last_season', 'current_club_id', 'height_in_cm', 'highest_market_value_in_eur', 'n_games', 'total_yellow', 'total_red', 'age']\n",
      "Categorical columns that will be one‐hot’ed: ['country_of_birth', 'city_of_birth', 'country_of_citizenship', 'sub_position', 'position', 'foot', 'current_club_domestic_competition_id']\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:39:02.837880Z",
     "start_time": "2025-06-04T13:39:01.154288Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# ─── 4. Fill/missing + scale numerics, then one‐hot encode categoricals ─────────────\n",
    "\n",
    "# Fill missing and scale numeric features\n",
    "X_num, scaler = fillna_and_scale(main_df, num_cols)\n",
    "\n",
    "# One‐hot encode all categorical features\n",
    "X_cat, encoder = encode_categorical(main_df, cat_cols)\n",
    "\n",
    "# Combine into one matrix\n",
    "import numpy as np\n",
    "\n",
    "if X_cat.shape[1] > 0:\n",
    "    X_full = np.hstack([X_num, X_cat.values])\n",
    "else:\n",
    "    X_full = X_num\n",
    "\n",
    "# Target (no NaN, because we dropped them above)\n",
    "y = main_df['market_value_in_eur']\n",
    "\n",
    "\n"
   ],
   "id": "92a59307531ef4e2",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T13:42:11.319425Z",
     "start_time": "2025-06-04T13:39:06.157355Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# ─── 5. Fit GBM & select “above‐median” features ─────────────────────────────────────\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "gbr = GradientBoostingRegressor(random_state=42)\n",
    "gbr.fit(X_full, y)\n",
    "\n",
    "selector   = SelectFromModel(gbr, prefit=True, threshold='median')\n",
    "X_selected = selector.transform(X_full)\n",
    "\n",
    "print(\"Total features before selection:\", X_full.shape[1])\n",
    "print(\"Selected features (importance ≥ median):\", X_selected.shape[1])\n"
   ],
   "id": "8aeb3a3f674aceb0",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 7\u001B[39m\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msklearn\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mfeature_selection\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m SelectFromModel\n\u001B[32m      6\u001B[39m gbr = GradientBoostingRegressor(random_state=\u001B[32m42\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m7\u001B[39m \u001B[43mgbr\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_full\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      9\u001B[39m selector   = SelectFromModel(gbr, prefit=\u001B[38;5;28;01mTrue\u001B[39;00m, threshold=\u001B[33m'\u001B[39m\u001B[33mmedian\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m     10\u001B[39m X_selected = selector.transform(X_full)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\amlfinal\\Lib\\site-packages\\sklearn\\base.py:1389\u001B[39m, in \u001B[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[39m\u001B[34m(estimator, *args, **kwargs)\u001B[39m\n\u001B[32m   1382\u001B[39m     estimator._validate_params()\n\u001B[32m   1384\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[32m   1385\u001B[39m     skip_parameter_validation=(\n\u001B[32m   1386\u001B[39m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[32m   1387\u001B[39m     )\n\u001B[32m   1388\u001B[39m ):\n\u001B[32m-> \u001B[39m\u001B[32m1389\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\amlfinal\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:787\u001B[39m, in \u001B[36mBaseGradientBoosting.fit\u001B[39m\u001B[34m(self, X, y, sample_weight, monitor)\u001B[39m\n\u001B[32m    784\u001B[39m     \u001B[38;5;28mself\u001B[39m._resize_state()\n\u001B[32m    786\u001B[39m \u001B[38;5;66;03m# fit the boosting stages\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m787\u001B[39m n_stages = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_fit_stages\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    788\u001B[39m \u001B[43m    \u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    789\u001B[39m \u001B[43m    \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    790\u001B[39m \u001B[43m    \u001B[49m\u001B[43mraw_predictions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    791\u001B[39m \u001B[43m    \u001B[49m\u001B[43msample_weight_train\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    792\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_rng\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    793\u001B[39m \u001B[43m    \u001B[49m\u001B[43mX_val\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    794\u001B[39m \u001B[43m    \u001B[49m\u001B[43my_val\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    795\u001B[39m \u001B[43m    \u001B[49m\u001B[43msample_weight_val\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    796\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbegin_at_stage\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    797\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmonitor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    798\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    800\u001B[39m \u001B[38;5;66;03m# change shape of arrays after fit (early-stopping or additional ests)\u001B[39;00m\n\u001B[32m    801\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m n_stages != \u001B[38;5;28mself\u001B[39m.estimators_.shape[\u001B[32m0\u001B[39m]:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\amlfinal\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:883\u001B[39m, in \u001B[36mBaseGradientBoosting._fit_stages\u001B[39m\u001B[34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001B[39m\n\u001B[32m    876\u001B[39m         initial_loss = factor * \u001B[38;5;28mself\u001B[39m._loss(\n\u001B[32m    877\u001B[39m             y_true=y_oob_masked,\n\u001B[32m    878\u001B[39m             raw_prediction=raw_predictions[~sample_mask],\n\u001B[32m    879\u001B[39m             sample_weight=sample_weight_oob_masked,\n\u001B[32m    880\u001B[39m         )\n\u001B[32m    882\u001B[39m \u001B[38;5;66;03m# fit next stage of trees\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m883\u001B[39m raw_predictions = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_fit_stage\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    884\u001B[39m \u001B[43m    \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    885\u001B[39m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    886\u001B[39m \u001B[43m    \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    887\u001B[39m \u001B[43m    \u001B[49m\u001B[43mraw_predictions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    888\u001B[39m \u001B[43m    \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    889\u001B[39m \u001B[43m    \u001B[49m\u001B[43msample_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    890\u001B[39m \u001B[43m    \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    891\u001B[39m \u001B[43m    \u001B[49m\u001B[43mX_csc\u001B[49m\u001B[43m=\u001B[49m\u001B[43mX_csc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    892\u001B[39m \u001B[43m    \u001B[49m\u001B[43mX_csr\u001B[49m\u001B[43m=\u001B[49m\u001B[43mX_csr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    893\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    895\u001B[39m \u001B[38;5;66;03m# track loss\u001B[39;00m\n\u001B[32m    896\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m do_oob:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\amlfinal\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:489\u001B[39m, in \u001B[36mBaseGradientBoosting._fit_stage\u001B[39m\u001B[34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001B[39m\n\u001B[32m    486\u001B[39m     sample_weight = sample_weight * sample_mask.astype(np.float64)\n\u001B[32m    488\u001B[39m X = X_csc \u001B[38;5;28;01mif\u001B[39;00m X_csc \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m X\n\u001B[32m--> \u001B[39m\u001B[32m489\u001B[39m \u001B[43mtree\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    490\u001B[39m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mneg_g_view\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m=\u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcheck_input\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\n\u001B[32m    491\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    493\u001B[39m \u001B[38;5;66;03m# update tree leaves\u001B[39;00m\n\u001B[32m    494\u001B[39m X_for_tree_update = X_csr \u001B[38;5;28;01mif\u001B[39;00m X_csr \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m X\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\amlfinal\\Lib\\site-packages\\sklearn\\base.py:1389\u001B[39m, in \u001B[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[39m\u001B[34m(estimator, *args, **kwargs)\u001B[39m\n\u001B[32m   1382\u001B[39m     estimator._validate_params()\n\u001B[32m   1384\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[32m   1385\u001B[39m     skip_parameter_validation=(\n\u001B[32m   1386\u001B[39m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[32m   1387\u001B[39m     )\n\u001B[32m   1388\u001B[39m ):\n\u001B[32m-> \u001B[39m\u001B[32m1389\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\amlfinal\\Lib\\site-packages\\sklearn\\tree\\_classes.py:1404\u001B[39m, in \u001B[36mDecisionTreeRegressor.fit\u001B[39m\u001B[34m(self, X, y, sample_weight, check_input)\u001B[39m\n\u001B[32m   1374\u001B[39m \u001B[38;5;129m@_fit_context\u001B[39m(prefer_skip_nested_validation=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m   1375\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mfit\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y, sample_weight=\u001B[38;5;28;01mNone\u001B[39;00m, check_input=\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[32m   1376\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001B[39;00m\n\u001B[32m   1377\u001B[39m \n\u001B[32m   1378\u001B[39m \u001B[33;03m    Parameters\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   1401\u001B[39m \u001B[33;03m        Fitted estimator.\u001B[39;00m\n\u001B[32m   1402\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1404\u001B[39m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1405\u001B[39m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1406\u001B[39m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1407\u001B[39m \u001B[43m        \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m=\u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1408\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcheck_input\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcheck_input\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1409\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1410\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\amlfinal\\Lib\\site-packages\\sklearn\\tree\\_classes.py:472\u001B[39m, in \u001B[36mBaseDecisionTree._fit\u001B[39m\u001B[34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001B[39m\n\u001B[32m    461\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    462\u001B[39m     builder = BestFirstTreeBuilder(\n\u001B[32m    463\u001B[39m         splitter,\n\u001B[32m    464\u001B[39m         min_samples_split,\n\u001B[32m   (...)\u001B[39m\u001B[32m    469\u001B[39m         \u001B[38;5;28mself\u001B[39m.min_impurity_decrease,\n\u001B[32m    470\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m472\u001B[39m \u001B[43mbuilder\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbuild\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtree_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmissing_values_in_feature_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    474\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.n_outputs_ == \u001B[32m1\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m is_classifier(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    475\u001B[39m     \u001B[38;5;28mself\u001B[39m.n_classes_ = \u001B[38;5;28mself\u001B[39m.n_classes_[\u001B[32m0\u001B[39m]\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# ─── 6. (Optional) Print exactly which feature names survived ────────────────────────\n",
    "\n",
    "# Build the full list of feature‐names in the same order that GBM saw them:\n",
    "feature_names = num_cols + X_cat.columns.tolist()\n",
    "\n",
    "# selector.get_support() is a boolean mask of length len(feature_names)\n",
    "kept_mask          = selector.get_support()\n",
    "kept_feature_names = [name for name, keep in zip(feature_names, kept_mask) if keep]\n",
    "\n",
    "print(\"\\nFeatures kept by SelectFromModel (≥ median importance):\")\n",
    "for feat in kept_feature_names:\n",
    "    print(\"  └\", feat)\n"
   ],
   "id": "1f068873a18b93c5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "model.fit(X_train, y_train, validation_split=0.2, epochs=50, batch_size=32, callbacks=[early_stop])\n",
    "\n",
    "y_pred = model.predict(X_test).ravel()\n",
    "scatter_actual_vs_pred(y_test, y_pred)\n"
   ],
   "id": "463385be9c2bc6b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "# Cell 6: Make predictions, clip extremes, compute metrics\n",
    "\n",
    "# 1. Predict on the test set (log-space)\n",
    "y_pred_log = model.predict(X_test).flatten()\n",
    "\n",
    "# 2. Clip log-predictions to avoid absurd expm1 outputs\n",
    "max_real_value = 12e7  # €300 million cap\n",
    "max_log = np.log1p(max_real_value)\n",
    "y_pred_log_clipped = np.clip(y_pred_log, a_min=0, a_max=max_log)\n",
    "\n",
    "# 3. Convert back to real-EUR scale\n",
    "y_test_exp = np.expm1(y_test)\n",
    "y_pred_exp = np.expm1(y_pred_log_clipped)\n",
    "\n",
    "# 4. Compute MAE and R^2 on the real-EUR scale\n",
    "mae_real = mean_absolute_error(y_test_exp, y_pred_exp)\n",
    "r2_real = r2_score(y_test_exp, y_pred_exp)\n",
    "\n",
    "# 5. Compute R^2 in log-space\n",
    "r2_log = r2_score(y_test, y_pred_log)\n",
    "\n",
    "print(f\"Test MAE (EUR scale, clipped): €{mae_real:,.2f}\")\n",
    "print(f\"Test R^2 (EUR scale, clipped): {r2_real:.3f}\")\n",
    "print(f\"Test R^2 (log1p scale): {r2_log:.3f}\")"
   ],
   "id": "6bed3f82bd38a527"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Cell 7: Scatter plot of Actual vs. Predicted (real-EUR), filtering out extreme predictions\n",
    "\n",
    "# Only plot points where predicted < €500M, so axes stay readable\n",
    "limit = 3e8\n",
    "mask_plot = y_pred_exp < limit\n",
    "\n",
    "# Apply the same mask to actuals so both arrays align\n",
    "x_vals = y_test_exp[mask_plot]\n",
    "y_vals = y_pred_exp[mask_plot]\n",
    "\n",
    "# Compute min and max for X and Y\n",
    "x_min, x_max = x_vals.min(), x_vals.max()\n",
    "y_min, y_max = y_vals.min(), y_vals.max()\n",
    "\n",
    "# Stretch limits just a tiny bit so points on the border aren’t cut off\n",
    "x_pad = (x_max / x_min) ** 0.05  \n",
    "y_pad = (y_max / y_min) ** 0.05\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(x_vals, y_vals, alpha=0.3)\n",
    "\n",
    "# Set log–log scale\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "\n",
    "# Set axis limits to exactly cover the data (with a tiny 1% padding)\n",
    "plt.xlim(x_min / x_pad, x_max * x_pad)\n",
    "plt.ylim(y_min / y_pad, y_max * y_pad)\n",
    "\n",
    "# Draw the y = x reference line over that same range\n",
    "# On a log–log plot, the diagonal line from (min, min) to (max, max) remains straight.\n",
    "plt.plot(\n",
    "    [x_min, x_max],\n",
    "    [x_min, x_max],\n",
    "    color='green',\n",
    "    linewidth=1,\n",
    "    linestyle='--',\n",
    "    zorder=0\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Actual Market Value (EUR)\")\n",
    "plt.ylabel(\"Predicted Market Value (EUR, clipped)\")\n",
    "plt.title(\"Actual vs. Predicted Market Value (axes fit to data)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "7cd1e6b66bd842cc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Example: try different layer sizes\n",
    "results = []\n",
    "for size in [16, 32, 64, 128]:\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(X_train.shape[1],)),\n",
    "        tf.keras.layers.Dense(size, activation='relu'),\n",
    "        tf.keras.layers.Dense(size, activation='relu'),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=0)\n",
    "    y_pred = model.predict(X_test).flatten()\n",
    "    y_pred_exp = np.expm1(y_pred)\n",
    "    mae = mean_absolute_error(y_test_exp, y_pred_exp)\n",
    "    results.append((size, mae))\n",
    "\n",
    "print(\"Layer size vs MAE:\")\n",
    "for size, mae in results:\n",
    "    print(f\"  {size} units: MAE = €{mae:,.2f}\")\n"
   ],
   "id": "ef4decb737a918bf"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (amlfinal)",
   "language": "python",
   "name": "amlfinal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
